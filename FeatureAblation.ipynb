{"cells":[{"cell_type":"markdown","source":["#Feature Ablation\n","\n","This Jupyter Notebook is demonstrating how to use the captum library to perform feature ablation on an image using a pre-trained VGG16 model in PyTorch. Feature ablation is a technique used to determine the importance of different parts of an input with respect to the model's output. The attribution scores obtained from feature ablation can be visualized using the captum.attr.visualization module.\n","\n","The first part of the code imports necessary libraries, loads and modifies the pre-trained VGG16 model, and defines the image transformation. Then, the input image is loaded and preprocessed using the defined transformation, and feature ablation is performed on the input image using the initialized FeatureAblation object.\n","\n","The second part of the code reshapes and normalizes the computed feature attribution values, and then visualizes the heatmap of the attributions using the captum.attr.visualization module. The visualization is created using the heat_map method and the resulting heatmap is displayed using Matplotlib's imshow and plt.show methods. The title and colorbar are added to the visualization to provide additional information.\n","\n","This Jupyter notebook is based on:\n","\n","* https://captum.ai/tutorials/Resnet_TorchVision_Ablation\n","* https://captum.ai/docs/attribution_algorithms#feature-ablation\n","* http://arxiv.org/abs/1910.00174"],"metadata":{"id":"TDTGVsgv5DQN"}},{"cell_type":"markdown","source":["#### Installing Captum Library\n","\n","The following line installs version 0.6.0 of the Captum library.\n","\n","This version is used because it is compatible with the other libraries used in this notebook."],"metadata":{"id":"kXHB2QOw5NJF"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712},"executionInfo":{"elapsed":17412,"status":"ok","timestamp":1691426953381,"user":{"displayName":"Patrick Döring","userId":"00751407020289025982"},"user_tz":-120},"id":"Ik5uRYiYk0lO","outputId":"5587c2f9-a235-4fa3-c8de-f3202b04d168"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting matplotlib==3.6\n","  Downloading matplotlib-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting captum==0.6.0\n","  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (4.41.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (1.4.4)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6) (2.8.2)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.6) (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum==0.6.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum==0.6.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum==0.6.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum==0.6.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum==0.6.0) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum==0.6.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum==0.6.0) (1.3.0)\n","Installing collected packages: matplotlib, captum\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","Successfully installed captum-0.6.0 matplotlib-3.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]}}},"metadata":{}}],"source":["!pip install matplotlib==3.6 captum==0.6.0"]},{"cell_type":"markdown","source":["#### Configuring Matplotlib for Displaying Images\n","\n","The following line configures Matplotlib to display images without padding.\n","\n","This configuration is necessary because Matplotlib adds padding around images by default, which can affect the layout of the notebook. Setting the `bbox_inches` parameter to `None` removes the padding and ensures that images are displayed at their original size without any cropping or scaling."],"metadata":{"id":"IGvO9szS5E8a"}},{"cell_type":"code","source":["%config InlineBackend.print_figure_kwargs={'bbox_inches':None}"],"metadata":{"id":"LhEV3i0JsVk9","executionInfo":{"status":"ok","timestamp":1691426953387,"user_tz":-120,"elapsed":42,"user":{"displayName":"Patrick Döring","userId":"00751407020289025982"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### Computing feature attribution using ablation method\n","A pre-trained VGG16 model is loaded and modified by replacing the input layer and output layer to adapt to the specific problem.\n","\n","The image transformation is defined and the image is loaded and preprocessed using the defined transformation. Then, a FeatureAblation object is initialized and feature ablation is performed on the input image using this object.\n","\n","Finally, the shape of the computed feature attribution is printed."],"metadata":{"id":"PrbhAdPP5FqE"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lacLQUnhAZQA","executionInfo":{"status":"ok","timestamp":1691426969579,"user_tz":-120,"elapsed":16231,"user":{"displayName":"Patrick Döring","userId":"00751407020289025982"}},"outputId":"ac2b5381-7bc6-46ef-c49f-bc3e94a71960"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","import torch\n","from torchvision import models, transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from captum.attr import FeatureAblation\n","import torch.nn.functional as F\n","\n","# Check GPU availability\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available.\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available, using CPU.\")\n","\n","# Load the pre-trained VGG16 model with modified input layer\n","model = models.vgg16(pretrained=True)\n","model = model.to(device)  # Move the model to the GPU if available\n","model.eval()\n","\n","transform = transforms.Compose([\n","    transforms.Resize(64),\n","    transforms.ToTensor()\n","])\n","\n","transform_normalize = transforms.Normalize(\n","    mean=[0.485, 0.456, 0.406],\n","    std=[0.229, 0.224, 0.225]\n",")\n","\n","image = Image.open('/content/drive/MyDrive/Colab Notebooks/images/school_bus.jpg')\n","\n","transformed_img = transform(image)\n","\n","input_img = transform_normalize(transformed_img)\n","input_img = input_img.unsqueeze(0)\n","input_img = input_img.to(device)  # Move the input data to the GPU if available\n","\n","target = torch.argmax(model(input_img))\n","print(\"Target class index:\", target)\n","\n","# Make predictions with the model\n","outputs = model(input_img)\n","output_probs = F.softmax(outputs, dim=1).squeeze(0)\n","# Find the index of the maximum value\n","label_idx = output_probs.argmax()\n","# Get the corresponding probability\n","probability = output_probs[label_idx].item() * 100 # converting to percentage\n","print(f\"Label Index: {label_idx}, Probability: {probability:.2f}%\")\n","\n","# Define the baseline input\n","baseline = torch.zeros_like(input_img)\n","baseline = baseline.to(device)  # Move the baseline data to the GPU if available\n","\n","# Initialize the FeatureAblation object\n","ablator = FeatureAblation(model)\n","\n","# Compute the feature attribution using the ablation method\n","attributions = ablator.attribute(inputs=input_img, baselines=baseline)\n","\n","print(\"Attributions shape:\", attributions.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uw9uNzBGpxRk","outputId":"5832e2d5-37b9-458c-e7d2-21435ecd28cc","executionInfo":{"status":"ok","timestamp":1691427115678,"user_tz":-120,"elapsed":142014,"user":{"displayName":"Patrick Döring","userId":"00751407020289025982"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:05<00:00, 100MB/s] \n"]},{"output_type":"stream","name":"stdout","text":["Target class index: tensor(779, device='cuda:0')\n","Label Index: 779, Probability: 100.00%\n","Attributions shape: torch.Size([1000, 3, 64, 85])\n"]}]},{"cell_type":"markdown","source":["#### XAI metrics\n","\n","A low infidelity value close to 0 indicates that the explanation provided by the XAI method is relatively faithful to the model's behavior. It suggests that the generated explanation is a good representation of how the model arrived at its predictions, making it a reliable and accurate explanation.\n","\n","A Sensitivity value close to 1 indicates that the model is highly sensitive to changes in specific input features. In other words, small changes in those features can lead to substantial changes in the model's predictions. This suggests that the model heavily relies on the identified features to make its decisions."],"metadata":{"id":"bL5A4mII8X4Y"}},{"cell_type":"code","source":["from captum.metrics import infidelity, sensitivity_max\n","import numpy as np\n","\n","# Assuming your original attributions tensor has the shape torch.Size([1000, 3, 64, 85])\n","\n","# Calculate the mean along the batch dimension (axis 0) to get the averaged attributions\n","averaged_attributions = torch.mean(attributions, dim=0, keepdim=True)\n","\n","# The shape of averaged_attributions will be torch.Size([1, 3, 64, 85])\n","print(averaged_attributions.shape)\n","\n","# Define the perturbation function\n","def perturb_fn(inputs):\n","    # Multiplying by 0.003 scales the random values to control the magnitude of the perturbations.\n","    noise = torch.tensor(np.random.normal(0, 0.003, inputs.shape)).float().to(device)\n","    return noise.to(device), (inputs - noise).to(device)\n","\n","# Compute the infidelity\n","infid = infidelity(model, perturb_fn, input_img, averaged_attributions, target=target)\n","\n","print(f\"Infidelity: {infid.item()}\")\n","\n","# Wrapper function for lr_lime.attribute\n","def fa_attr(inputs):\n","    return ablator.attribute(inputs)\n","\n","# Calculate Sensitivity\n","sens = sensitivity_max(fa_attr, input_img)\n","print(f\"Sensitivity: {sens.item()}\")"],"metadata":{"id":"Mi-qLg2T8YH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690271357101,"user_tz":-120,"elapsed":675168,"user":{"displayName":"Patrick Döring","userId":"00751407020289025982"}},"outputId":"18dcdbed-c980-4306-fbcc-5154ceede7b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 64, 85])\n","Infidelity: 0.007453133352100849\n","Sensitivity: 9.630292892456055\n"]}]},{"cell_type":"markdown","source":["#### Visualization\n","This cell is visualizing the feature attribution results obtained from the previous code block using the captum.attr.visualization module.\n","\n","First, the computed feature attribution is reshaped to match the size of the input image, and then its absolute values are taken. The heatmap shape and the minimum and maximum values of the attributions are printed.\n","\n","Next, the heatmap values are normalized to the range [0, 255] and converted to a numpy array of unsigned 8-bit integers. The original image is plotted first using Matplotlib's imshow method. Then, the heatmap of the attributions is visualized using the visualize_image_attr method from the captum.attr.visualization module. The visualization is shown using Matplotlib's plt.show() method.\n","\n","The heat_map method is used to create the visualization with a color map specified as 'jet'. The 'all' sign is specified to show both positive and negative contributions. The title 'Feature Ablation' is also added to the visualization. The show_colorbar parameter is set to True to display a colorbar indicating the attribution values."],"metadata":{"id":"FlH0As5D5KTQ"}},{"cell_type":"code","source":["from torchvision import transforms\n","import matplotlib.colors as colors\n","from captum.attr import visualization as viz\n","\n","# Reshape the attributions to match the image size\n","attributions = attributions.squeeze()\n","if len(attributions.shape) == 4:\n","    attributions = attributions[0]\n","attributions = np.transpose(attributions, (1, 2, 0))\n","attributions = np.abs(attributions)\n","\n","print(\"Heatmap shape:\", attributions.shape)\n","print(\"Attributions min/max:\", attributions.min(), attributions.max())\n","\n","# Normalize the heatmap values to the range [0, 255]\n","attributions_norm = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n","attributions_norm = (attributions_norm * 255).numpy().astype(np.uint8)\n","\n","# Plot the heatmap of the attributions\n","plt.imshow(image.resize((64, 64)))\n","plt.axis('off')\n","plt.title('Input Image')\n","plt.show()\n","\n","# Visualize the heatmap of the attributions\n","_ = viz.visualize_image_attr(attributions_norm,\n","                             method=\"heat_map\",\n","                             cmap=\"RdBu_r\",\n","                             show_colorbar=True,\n","                             title='Feature Ablation')\n","\n","plt.show()\n"],"metadata":{"id":"GA7rUT4eC2v_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1kZcVhoh576NIjAvfokiQF8JS1dHbuB2_","authorship_tag":"ABX9TyO8Zfq4WTOYIWPu/TZpHaDZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}