# Explainable AI and Computer Vision Techniques for Interpretable and Safe Autonomous Driving

This repository contains a collection of Jupyter notebooks that demonstrate various techniques for explainable AI and computer vision in the context of autonomous driving. These notebooks were created using Google Colaboratory.

## Notebooks

- `CaptionGeneration.ipynb`: This notebook demonstrates how to generate captions for images using a deep learning model.
- `DeepLift.ipynb`: This notebook shows how to use the DeepLift algorithm to interpret the predictions of a convolutional neural network.
- `FeatureAblation.ipynb`: This notebook demonstrates how to perform feature ablation to understand the importance of different features in a deep learning model.
- `FilterFeatureVisualization.ipynb`: This notebook shows how to visualize the filters and features learned by a convolutional neural network.
- `GradCam.ipynb`: This notebook demonstrates how to use Grad-CAM to generate class activation maps that highlight the important regions of an image for a given class.
- `IntegratedGradientsGradientShapOcclusionNoiseTunnel.ipynb`: This notebook shows how to use various attribution methods, such as integrated gradients, gradient SHAP, occlusion, and noise tunnel, to explain the predictions of a deep learning model.
- `LIME.ipynb`: This notebook demonstrates how to use LIME (Local Interpretable Model-agnostic Explanations) to explain the predictions of a deep learning model.
- `ModelVisualizer.ipynb`: This notebook shows how to visualize the architecture of a deep learning model using Graphviz.
- `TCAV.ipynb`: This notebook demonstrates how to use TCAV (Testing with Concept Activation Vectors) to interpret the predictions of a deep learning model.
- `TracInCPFast.ipynb`: This notebook shows how to use the TracInCPFast algorithm to interpret the predictions of a convolutional neural network.

## Usage

To use these notebooks, simply open them in Google Colaboratory and follow the instructions in each notebook.
